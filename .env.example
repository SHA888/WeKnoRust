# Usage Instructions
# 1. Copy this file to .env
# 2. Replace placeholders with actual values
# 3. Ensure the .env file is not committed to version control

# gin mode
# Options: debug (development with detailed logs), release (production)
GIN_MODE=debug

# Base URL of the Ollama service, used to connect to a local/remote Ollama instance
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Storage configuration
# Primary database type (postgres/mysql)
DB_DRIVER=postgres

# Vector store type (postgres/elasticsearch_v7/elasticsearch_v8)
RETRIEVE_DRIVER=postgres

# File storage type (local/minio/cos)
STORAGE_TYPE=local

# Stream processing backend (memory/redis)
STREAM_MANAGER_TYPE=redis

# Primary database configuration
# Database port, default 5432
DB_PORT=5432

# Database username
DB_USER=postgres

# Database password
DB_PASSWORD=postgres123!@#

# Database name
DB_NAME=WeKnowRust

# If using Redis as the stream processing backend, configure the following
# Redis port, default 6379
REDIS_PORT=6379

# Redis password; leave empty if no password is set
REDIS_PASSWORD=redis123!@#

# Redis DB index, default 0
REDIS_DB=0

# Prefix for Redis keys, used for namespacing
REDIS_PREFIX=stream:

# Base directory for files when using local storage
LOCAL_STORAGE_BASE_DIR=./data/files

TENANT_AES_KEY=weknowrust-api-key-secret-secret

# Whether to enable Knowledge Graph construction and retrieval (construction requires LLM calls and takes time)
ENABLE_GRAPH_RAG=false

MINIO_PORT=9000

MINIO_CONSOLE_PORT=9001

# Embedding concurrency; lower this value if you encounter 429 rate limit errors
CONCURRENCY_POOL_SIZE=5

# If using ElasticSearch as the vector store, configure the following
# ElasticSearch address, e.g., http://localhost:9200
# ELASTICSEARCH_ADDR=your_elasticsearch_addr

# ElasticSearch username, if authentication is required
# ELASTICSEARCH_USERNAME=your_elasticsearch_username

# ElasticSearch password, if authentication is required
# ELASTICSEARCH_PASSWORD=your_elasticsearch_password

# ElasticSearch index name for storing vector data
# ELASTICSEARCH_INDEX=WeKnowRust

# If using MinIO for file storage, configure the following
# MinIO access key
# MINIO_ACCESS_KEY_ID=your_minio_access_key

# MinIO secret key
# MINIO_SECRET_ACCESS_KEY=your_minio_secret_key

# MinIO bucket name for storing files
# MINIO_BUCKET_NAME=your_minio_bucket_name

# If using Tencent Cloud COS for file storage, configure the following
# COS secret ID
# COS_SECRET_ID=your_cos_secret_id

# COS secret key
# COS_SECRET_KEY=your_cos_secret_key

# COS region, e.g., ap-guangzhou
# COS_REGION=your_cos_region

# COS bucket name
# COS_BUCKET_NAME=your_cos_bucket_name

# COS application ID
# COS_APP_ID=your_cos_app_id

# COS path prefix for storing files
# COS_PATH_PREFIX=your_cos_path_prefix

# COS_ENABLE_OLD_DOMAIN=true enables the legacy domain format; default true
COS_ENABLE_OLD_DOMAIN=true

# If you use a web proxy for network access, configure the following
# WEB_PROXY=your_web_proxy

##############################################################

###### Note: The following configurations are no longer effective; they are handled during the web "configuration initialization" phase #########


# # Initialize default tenant and knowledge base
# # Tenant ID, usually a string
# INIT_TEST_TENANT_ID=1

# # Knowledge base ID, usually a string
# INIT_TEST_KNOWLEDGE_BASE_ID=kb-00000001

# # LLM Model
# # Name of the LLM model
# # By default uses Ollama's Qwen3 8B model; Ollama will download/load automatically
# # Replace with another model name if needed
# INIT_LLM_MODEL_NAME=qwen3:8b

# # LLM model base URL
# # Supports third-party model service URLs
# # Leave empty when using local Ollama; it will be handled automatically
# # INIT_LLM_MODEL_BASE_URL=your_llm_model_base_url

# # LLM API key for authentication if required
# # Supports third-party model service API keys
# # Leave empty when using local Ollama
# # INIT_LLM_MODEL_API_KEY=your_llm_model_api_key

# # Embedding Model
# # Name of the embedding model
# # Default is nomic-embed-text for text embeddings
# # Replace with another model name if needed
# INIT_EMBEDDING_MODEL_NAME=nomic-embed-text

# # Embedding vector dimension
# INIT_EMBEDDING_MODEL_DIMENSION=768

# # Embedding model ID, usually a string
# INIT_EMBEDDING_MODEL_ID=builtin:nomic-embed-text:768

# # Embedding model base URL
# # Supports third-party model service URLs
# # Leave empty when using local Ollama
# # INIT_EMBEDDING_MODEL_BASE_URL=your_embedding_model_base_url

# # Embedding model API key for authentication if required
# # Supports third-party model service API keys
# # Leave empty when using local Ollama
# # INIT_EMBEDDING_MODEL_API_KEY=your_embedding_model_api_key

# # Rerank Model (optional)
# # For RAG, using a rerank model can significantly improve search accuracy
# # Ollama currently does not support running rerank models
# # Name of the rerank model
# # INIT_RERANK_MODEL_NAME=your_rerank_model_name

# # Rerank model base URL
# # Supports third-party model service URLs
# # INIT_RERANK_MODEL_BASE_URL=your_rerank_model_base_url

# # Rerank model API key for authentication if required
# # Supports third-party model service API keys
# # INIT_RERANK_MODEL_API_KEY=your_rerank_model_api_key

# # VLM_MODEL_NAME: name of the multimodal model used
# # Used for processing image data
# # VLM_MODEL_NAME=your_vlm_model_name

# # VLM_MODEL_BASE_URL: base URL for the multimodal model
# # Supports third-party model service URLs
# # VLM_MODEL_BASE_URL=your_vlm_model_base_url

# # VLM_MODEL_API_KEY: API key for the multimodal model
# # Supports third-party model service API keys
# # VLM_MODEL_API_KEY=your_vlm_model_api_key